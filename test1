def guid = UUID.randomUUID().toString()

pipeline {
  agent {
    kubernetes {
      defaultContainer 'aws-build-tools'
      yaml podTemplate()
      yamlMergeStrategy merge()
    }
  }

  options { disableResume() }

  stages {

    stage('Set Env Vars (FRR account & bucket)') {
      steps {
        script {
          def awsRoleName; def awsAccount; def s3BucketPrefix
          switch (params.AWS_ENVIRONMENT) {
            case 'nonprod':
              if (params.TRANSACT_ENVIRONMENT == 'feature-frr-aws') {
                awsRoleName    = 'frr-templt-nonprod-dev-jenkins'
                awsAccount     = '752177477252'
                s3BucketPrefix = 'frr-nonprod'
                break
              }
              if (params.TRANSACT_ENVIRONMENT == 'feature-test-aws') {
                awsRoleName    = 'frr-templt-nonprod-test-jenkins'
                awsAccount     = '752177477252'
                s3BucketPrefix = 'frr-nonprod'
                break
              }
            case 'preprod':
              awsRoleName    = 'frr-templt-preprod-ppte-jenkins'
              awsAccount     = '851725356859'
              s3BucketPrefix = 'frr-preprod'
              break
          }
          env.AWS_ROLE_NAME  = awsRoleName
          env.AWS_ACCOUNT    = awsAccount
          env.S3_BUCKET_PREFIX = s3BucketPrefix
        }
      }
    }

    stage('Resolve COB_DATE & locate CSV on S3') {
      steps {
        container('aws-build-tools') {
          script {
            if (!params.COB_DATE?.trim()) {
              def tz  = TimeZone.getTimeZone('Pacific/Auckland')
              def cal = Calendar.getInstance(tz); cal.add(Calendar.DATE, -1)
              env.COB_DATE = cal.format('yyyyMMdd')
              echo "Auto-set COB_DATE (NZT yesterday): ${env.COB_DATE}"
            } else {
              env.COB_DATE = params.COB_DATE.trim()
            }

            env.BUCKET_NAME = "${env.S3_BUCKET_PREFIX}-${params.TRANSACT_ENVIRONMENT}-extracts-${env.AWS_ACCOUNT}"
            env.LOCAL_CSV   = "VIEW_DDLS_${env.COB_DATE}.csv"
            env.REPORT_KEY  = "reports/${env.COB_DATE}/${env.LOCAL_CSV}"
            echo "S3 file => s3://${env.BUCKET_NAME}/${env.REPORT_KEY}"
          }
        }
      }
    }

    stage('Download CSV from FRR S3') {
      steps {
        container('aws-build-tools') {
          withBNZVault(secrets: [[type: 'vaultAWS', accountId: 'aws', role: env.AWS_ROLE_NAME]]) {
            sh '''#!/bin/bash
set -euo pipefail
command -v aws >/dev/null 2>&1 || { echo "aws cli not found"; exit 127; }

rm -f "$LOCAL_CSV" || true
aws s3 ls "s3://$BUCKET_NAME/$REPORT_KEY" >/dev/null
aws s3 cp "s3://$BUCKET_NAME/$REPORT_KEY" "$LOCAL_CSV"

echo "Downloaded -> $LOCAL_CSV"
wc -l "$LOCAL_CSV" || true
'''
          }
        }
      }
    }

    stage('Snowflake: PUT + COPY VIEW_DDLS') {
      steps {
        container('dap-build-tools') {
          withCredentials([sshUserPrivateKey(credentialsId: params.CREDENTIAL_ID, keyFileVariable: 'PRIVATE_KEY_PATH')]) {
            withEnv([
              "SNOW_ACCOUNT=${params.SNOW_ACCOUNT}",
              "SNOW_USER=${params.SNOW_USER}",
              "SNOW_ROLE=${params.SNOW_ROLE}",
              "SNOW_WAREHOUSE=${params.SNOW_WH}",
              "SNOW_DATABASE=${params.SNOW_DB}",
              "SNOW_SCHEMA=${params.SNOW_SCHEMA}",
              "SNOW_TABLE=${params.SNOW_TABLE ?: 'VIEW_DDLS'}",
              "CSV_DELIM=${params.CSV_DELIM ?: '~'}",     // change via param if comma CSV
              "FORCE_COPY=${params.FORCE_COPY ?: 'FALSE'}"
            ]) {
              sh '''#!/bin/bash
set -euo pipefail

command -v snowsql >/dev/null 2>&1 || { echo "snowsql not found"; exit 127; }

FILE_BASENAME="$LOCAL_CSV"
TARGET_FQN="${SNOW_DATABASE}.${SNOW_SCHEMA}.${SNOW_TABLE}"

# ensure file exists
[ -f "$FILE_BASENAME" ] || { echo "CSV not found: $FILE_BASENAME"; ls -la; exit 66; }

# build COPY script
cat > copy.sql <<SQL
USE ROLE ${SNOW_ROLE};
USE WAREHOUSE ${SNOW_WAREHOUSE};
USE DATABASE ${SNOW_DATABASE};
USE SCHEMA ${SNOW_SCHEMA};

-- do not overwrite existing staged file version (lets FORCE=FALSE truly skip)
PUT file://$PWD/${FILE_BASENAME} @~ OVERWRITE=FALSE AUTO_COMPRESS=FALSE;

COPY INTO ${TARGET_FQN}
FROM @~/${FILE_BASENAME}
FILE_FORMAT = (
  TYPE = CSV
  FIELD_DELIMITER = '${CSV_DELIM}'
  SKIP_HEADER = 1
  FIELD_OPTIONALLY_ENCLOSED_BY = '"'
  TRIM_SPACE = TRUE
  SKIP_BYTE_ORDER_MARK = TRUE
  EMPTY_FIELD_AS_NULL = TRUE
)
MATCH_BY_COLUMN_NAME = CASE_INSENSITIVE
ON_ERROR = ABORT_STATEMENT
FORCE = ${FORCE_COPY};

-- quick peek
SELECT * FROM ${TARGET_FQN} ORDER BY 1 DESC LIMIT 5;
SQL

echo "===== copy.sql (head) ====="; sed -n '1,180p' copy.sql || true; echo "===== end ====="

snowsql \
  -a "$SNOW_ACCOUNT" -u "$SNOW_USER" -r "$SNOW_ROLE" \
  -w "$SNOW_WAREHOUSE" -d "$SNOW_DATABASE" -s "$SNOW_SCHEMA" \
  -f copy.sql -o friendly=false -o exit_on_error=true
'''
            }
          }
        }
      }
    }
  }

  post {
    success {
      echo "Loaded ${env.LOCAL_CSV} (COB_DATE=${env.COB_DATE}) into ${params.SNOW_DB}.${params.SNOW_SCHEMA}.${params.SNOW_TABLE ?: 'VIEW_DDLS'}"
    }
  }
}

/* ---- Pod template: same image as your other pipelines ---- */
String podTemplate() {
  return """
apiVersion: "v1"
kind: "Pod"
spec:
  containers:
  - args: [ "99d" ]
    command: [ "sleep" ]
    image: "registry.nz.thenational.com/cicd/build/aws-build-tools:1.3"
    imagePullPolicy: "Always"
    name: "aws-build-tools"
    env:
    - name: HTTPS_PROXY
      value: "http://proxy.bnz.co.nz:10568"
    - name: no_proxy
      value: ".nz.thenational.com"
    resources:
      limits: { memory: "512Mi", cpu: "1" }
      requests: { memory: "128Mi", cpu: "0.1", ephemeral-storage: "64Mi" }
    securityContext:
      runAsUser: 1001
"""
}

